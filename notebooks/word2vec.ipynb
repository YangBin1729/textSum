{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练词向量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 利用`gensim.models.word2vec`训练词向量\n",
    "- 原始语料为[中文维基](https://dumps.wikimedia.org/zhwiki/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. 原始语料为 `xml` 格式，需要提取出正文，使用 `WikiExtractor` 包\n",
    "    0. 命令行提取正文：`python WikiExtractor.py -b 500M -o wiki zhwiki-20190720-pages-articles-multistream.xml.bz2`\n",
    "    0. 获得的文件中，正文被包含在 `<doc></doc>` 标签内\n",
    "0. 或者 `gensim.corpora.WikiCorpus` 直接处理 `xml.bz2` 文件\n",
    "0. 由上两步，获得的文本先经过预处理，**每一行一句话，单词间用空格隔开**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python WikiExtractor.py -b 500M -o datasets/wiki datasets/zhwiki-20190720.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_zhwiki_v1():\n",
    "    # 提取文本信息，分句、分词、繁体转简体，然后将单词用空格连接\n",
    "    regex = re.compile(\"(^<doc.*>$)|(^</doc>$)\")\n",
    "    sent_spliter = re.compile(\"。|！|？\")\n",
    "\n",
    "    input_file = open(input_file_path, 'r', encoding='utf-8')\n",
    "    output_file = open(output_file_path, 'w+', encoding='utf-8')\n",
    "\n",
    "    line = input_file.readline()\n",
    "    while line:\n",
    "        if line.strip() and not regex.match(line):\n",
    "            sentences = sent_spliter.split(line)\n",
    "            for s in sentences:\n",
    "                s = zhconv.convert(s, 'zh-cn')\n",
    "                words = jieba.cut(s.strip('\\n'))\n",
    "                sent = ' '.join(words)\n",
    "                output_file.write(sent + '\\n')\n",
    "        line = input_file.readline()\n",
    "\n",
    "    input_file.close()\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "\n",
    "def preprocess_zhwiki_v2():\n",
    "    # 提取文本信息，分句、分词、繁体转简体，然后将单词用空格连接\n",
    "    # WikiCorpus 会将标点符号都被删除\n",
    "    space = ' '\n",
    "    i = 0\n",
    "    l = []\n",
    "\n",
    "    output_file = open(output_file_path, 'w+', encoding='utf-8')\n",
    "\n",
    "    wiki = WikiCorpus(input_file_path, lemmatize=False, dictionary={})\n",
    "    for text in wiki.get_texts():\n",
    "        for temp_sentence in text:\n",
    "            temp_sentence = zhconv.convert(s, 'zh-cn')\n",
    "            seg_list = list(jieba.cut(temp_sentence))\n",
    "            for temp_term in seg_list:\n",
    "                l.append(temp_term)\n",
    "        output_file.write(space.join(l) + '\\n')\n",
    "        l = []\n",
    "        i = i + 1\n",
    "\n",
    "        if (i % 200 == 0):\n",
    "            print('Saved ' + str(i) + ' articles')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "input_file_path = r'datasets/wiki/AA/wiki_00'\n",
    "output_file_path = r'datasets/wiki/AA/wiki_corpus'\n",
    "preprocess_zhwiki_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 利用上一步生成的处理后的满足 `LineSentence` 格式的文本，创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "corpus_path = output_file_path\n",
    "model_path = r\"models/wiki_corpus.model\"\n",
    "\n",
    "\n",
    "def build_model(corpus_path):\n",
    "    wiki_news = word2vec.LineSentence(corpus_path)\n",
    "    model = word2vec.Word2Vec(\n",
    "        wiki_news,\n",
    "        sg=0,  # 模型类型 CBOW\n",
    "        size=50,  # 词向量维度     \n",
    "        window=5,  # 窗口尺寸\n",
    "        min_count=5, # 忽略词频少于 5 的单词\n",
    "        workers=9)\n",
    "    model.save(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 验证训练得到的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model_path = r\"models/zhwiki.50d.word2vec\"\n",
    "model = word2vec.Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('数学')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('哲学')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['女人', '国王'], negative=['男人'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_corpus = [\"腾讯\",\"阿里巴巴\"]\n",
    "res = model.wv.similarity(two_corpus[0],two_corpus[1])\n",
    "print(\"similarity:%.4f\"%res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将词向量降维后进行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "%matplotlib inline\n",
    "\n",
    "word_vectors = model.wv\n",
    "\n",
    "\n",
    "def get_model_matrix(word_vectors, required_words):\n",
    "    import random\n",
    "    words = list(word_vectors.vocab.keys())\n",
    "    random.shuffle(words)\n",
    "    words = words[:10000]\n",
    "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
    "    word2Ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(word_vectors.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for w in required_words:\n",
    "        try:\n",
    "            M.append(word_vectors.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "    return M, word2Ind\n",
    "\n",
    "\n",
    "words = [\n",
    "    '数学', '算术', '公理', '积分', '统计', '善恶', '哲学', '伦理', '中国政府', '美国国会', '武侠小说',\n",
    "    '风靡', '海内外', '受欢迎', '通俗小说', '中华人民共和国', '文化大革命', '反思', '伤痕', '一批', '白话文',\n",
    "    '诗人', '古诗', '欢迎', '中华民国', '撤退', '台湾', '区别', '思潮', '过渡时期', '通称', '文献', '兴趣',\n",
    "    '钻研', '语言学', '神秘主义', '更加', '经典', '历史学', '文学', '学术界', '享有', '前所未有', '趋势',\n",
    "    '受到', '人文主义者', '巨量', '规则', '机器人', '精准', '身躯', '脑', '视频', '确保', '高质量', '适中',\n",
    "    '价格', '软件设计', '构成', '互补', '并行', '系统分析', '程序设计', '支持', '高级', '课程', '训练',\n",
    "    '工业', '技能', '羧酸', '柠檬酸', '高效率', '肽键', '细胞骨架', '细胞周期', '氯仿', '甘油', '变型',\n",
    "    '鞘', '类固醇', '醛', '酮', '糖原', '单糖', '半乳糖', '葡萄糖', '糖苷键', '含氮', '杂环', '嘌呤',\n",
    "    '辅酶', '底物', '化学能', '磷酸化', '哈康', '延斯', '挪威海', '捕鲸', '挪威政府', '成人礼', '巴伦支海',\n",
    "    '哥德堡', '区域规划', '润州', '邳州市', '东海县', '丹阳市', '武进区', '临河', '嘈杂', '霰弹枪', '讲席',\n",
    "    '一滴', '调换', '香港金融管理局', '美圆', '金管局', '毫', '大额', '铜币', '一圆', '镍币', '爆竹',\n",
    "    '管理科', '中区', '收兑', '财政司'\n",
    "]\n",
    "\n",
    "M, word2Ind = get_model_matrix(word_vectors, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 svd 算法进行降维\n",
    "def reduce_to_k_dim(M, k=2):\n",
    "    n_iters = 10\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    print(\"Done.\")\n",
    "    return M_reduced\n",
    "\n",
    "M_reduced = reduce_to_k_dim(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced, word2Ind, words):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(24,24))\n",
    "    for word in words:\n",
    "        index = word2Ind[word]\n",
    "        x, y = M_reduced[index]\n",
    "        plt.scatter(x, y, marker='o', color='red')\n",
    "        plt.text(x, y, word, fontsize=9)\n",
    "        \n",
    "plot_embeddings(M_reduced, word2Ind, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 利用 TSNE 算法进行降维\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne_plot(M, word2Ind, words):\n",
    "\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    M_reduced = tsne_model.fit_transform(M)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(32, 32))\n",
    "    for word in words:\n",
    "        index = word2Ind[word]\n",
    "        x, y = M_reduced[index]\n",
    "        ax.scatter(x, y, marker='o', color='red')\n",
    "        ax.text(x, y, word, fontsize=9)\n",
    "        \n",
    "tsne_plot(M, word2Ind, words)\n",
    "\n",
    "# TSNE 降维效果比 SVD 要好，但效率更低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 关键词提取，从 `wv.most_similar()` 出发获取给定单词的相关单词\n",
    "    - 词向量 `wv.most_similar()` 获得的为出现在相似上下文中的同类词，并不是通常语义含义上的相似词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_related_words(initial_words, model):\n",
    "    unseen = [initial_words]\n",
    "    seen = defaultdict(int)\n",
    "\n",
    "    max_size = 500\n",
    "\n",
    "    while unseen and len(seen) < max_size:\n",
    "        if len(seen) % 50 == 0:\n",
    "            print('search length: {}'.format(len(seen)))\n",
    "\n",
    "        node = unseen.pop(0)\n",
    "        new_expanding = [w for w, _ in model.most_similar(node, topn=20)]\n",
    "        unseen += new_expanding\n",
    "\n",
    "        seen[node] += 1\n",
    "    return seen\n",
    "\n",
    "\n",
    "actions = get_related_words(\"说\", word_vectors)\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `wordcloud` 实现词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 培根散文集的词云\n",
    "data_path = r'datasets/Bacon Francis - Essays.txt'\n",
    "\n",
    "import os\n",
    "\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = open(data_path).read()\n",
    "\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "879.85px",
    "left": "433px",
    "right": "20px",
    "top": "120px",
    "width": "358.5px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
